{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "df1 = pd.read_csv('./datasets/labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "df1 = df1.drop(['id'], axis=1)\n",
    "df2 = pd.read_csv('./datasets/imdb_master.csv',encoding=\"latin-1\")\n",
    "df2['review'] = df2.review.apply(lambda x: clean_text(x))\n",
    "df2 = df2[df2.label != 'unsup']\n",
    "df2['label'].replace('neg', 0, inplace=True)\n",
    "df2['label'].replace('pos', 1, inplace=True)\n",
    "df2 = df2.drop(columns=[df2.keys()[0], df2.keys()[4]])\n",
    "df2 = df2.rename(columns={'label':'sentiment'})\n",
    "df2_train, df2_test = df2[df2['type']=='train'].drop(columns=['type']), df2[df2['type']=='test'].drop(columns=['type'])\n",
    "df2_train, df2_test = df2_train.reset_index(drop=True), df2_test.reset_index(drop=True)\n",
    "df2_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df2_train['review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df2_train['review'])\n",
    "\n",
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = df2_train['sentiment']\n",
    "\n",
    "embed_size = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "df1_test=pd.read_csv(\"./datasets/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "df1_test[\"sentiment\"] = df1_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "\n",
    "def model_performance_test(tested_model, df):\n",
    "    df['review'] = df.review.apply(lambda x: clean_text(x))\n",
    "    y_test = df[\"sentiment\"]\n",
    "    list_sentences_test = df[\"review\"]\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "    X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "    prediction = tested_model.predict(X_te)\n",
    "    y_pred = (prediction > 0.5)\n",
    "    print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_pred, y_test))\n",
    "    print('Accuracy: {0}'.format(accuracy_score(y_pred, y_test)))\n",
    "    return f1_score(y_pred, y_test), confusion_matrix(y_pred, y_test)\n",
    "\n",
    "model_performance_test(model, df2_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "similar_word_generator = api.load('glove-twitter-25')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def word_existence_oracle(sentence, word):\n",
    "    if word in sentence:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_existence_in_train(word, df):\n",
    "    result = 0\n",
    "    for sentence in df['review']:\n",
    "        result = word_existence_oracle(sentence, word)\n",
    "    return result\n",
    "\n",
    "def pick_similar_word(word, df):\n",
    "    for similar in similar_word_generator.most_similar(word, topn=10):\n",
    "        if not check_existence_in_train(similar[0], df):\n",
    "            return similar[0]\n",
    "    return None\n",
    "\n",
    "def compute_effect_of_word(word, df, df_train, tested_model):\n",
    "    if pick_similar_word(word, df_train) is None:\n",
    "        return None\n",
    "    else:\n",
    "        similar = pick_similar_word(word, df_train)\n",
    "    df_chosen = np.array(df['review'][[word in i for i in df['review']]])\n",
    "    prediction1 = tested_model.predict(pad_sequences(tokenizer.texts_to_sequences(df_chosen), maxlen=maxlen))\n",
    "    for i, sentence in enumerate(df_chosen):\n",
    "        df_chosen[i] = sentence.replace(word, similar)\n",
    "    prediction2 = tested_model.predict(pad_sequences(tokenizer.texts_to_sequences(df_chosen), maxlen=maxlen))\n",
    "    return prediction1 - prediction2\n",
    "\n",
    "movie_difference = compute_effect_of_word('movie', df2_test, df2_train, model)\n",
    "shot_difference = compute_effect_of_word('shot', df2_test, df2_train, model)\n",
    "contain_difference = compute_effect_of_word('contain', df2_test, df2_train, model)\n",
    "us_difference = compute_effect_of_word('us', df2_test, df2_train, model)\n",
    "it_difference = compute_effect_of_word('it', df2_test, df2_train, model)\n",
    "there_difference = compute_effect_of_word('there', df2_test, df2_train, model)\n",
    "be_difference = compute_effect_of_word('be', df2_test, df2_train, model)\n",
    "with_difference = compute_effect_of_word('with', df2_test, df2_train, model)\n",
    "house_difference = compute_effect_of_word('house', df2_test, df2_train, model)\n",
    "\n",
    "print('mean, var of \\'contain\\': ', np.mean(contain_difference), np.var(contain_difference))\n",
    "print('mean, var of \\'shot\\': ', np.mean(shot_difference), np.var(shot_difference))\n",
    "print('mean, var of \\'movie\\': ', np.mean(movie_difference), np.var(movie_difference))\n",
    "print('mean, var of \\'us\\': ', np.mean(us_difference), np.var(us_difference))\n",
    "print('mean, var of \\'it\\': ', np.mean(it_difference), np.var(it_difference))\n",
    "print('mean, var of \\'there\\': ', np.mean(there_difference), np.var(there_difference))\n",
    "print('mean, var of \\'be\\': ', np.mean(be_difference), np.var(be_difference))\n",
    "print('mean, var of \\'with\\': ', np.mean(with_difference), np.var(with_difference))\n",
    "print('mean, var of \\'house\\': ', np.mean(house_difference), np.var(house_difference))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "words_with_no_meaning = ['it', 'this', 'are', 'is', 'was', 'will', 'that', 'my', 'there', 'be', 'with', 'in',\n",
    "                         'out', 'on', 'under', 'how', 'what', 'why', 'may', 'have', 'where', 'he', 'she', 'do',\n",
    "                         'when', 'were', 'these', 'those', 'can', 'could', 'has', 'had', 'them', 'would', 'which']\n",
    "\n",
    "nouns_can_be_removed = ['movie', 'house', 'film', 'car', 'tree']\n",
    "\n",
    "biased_words = words_with_no_meaning\n",
    "\n",
    "def word_existence_list(word, df):\n",
    "    tmp_list = np.zeros(len(df['review']))\n",
    "    for count, sentence in enumerate(df['review']):\n",
    "        tmp_list[count] = float(word in sentence)\n",
    "    return tmp_list\n",
    "\n",
    "biased_features_with_no_meaning = pd.DataFrame()\n",
    "biased_features_with_no_meaning_test = pd.DataFrame()\n",
    "for words in words_with_no_meaning:\n",
    "    biased_features_with_no_meaning[words] = word_existence_list(words, df2_train)\n",
    "    biased_features_with_no_meaning_test[words] = word_existence_list(words, df2_test)\n",
    "clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "clf.fit(biased_features_with_no_meaning, df2_train['sentiment'])\n",
    "rf_prediction_without_probability = clf.predict(biased_features_with_no_meaning)\n",
    "rf_prediction_without_probability_test = clf.predict(biased_features_with_no_meaning_test)\n",
    "print('Train accuracy is ',\n",
    "      np.sum([y==df2_train['sentiment'][count]\n",
    "              for count, y in enumerate(rf_prediction_without_probability)])/len(rf_prediction_without_probability))\n",
    "print('Test accuracy is ',\n",
    "      np.sum([y==df2_test['sentiment'][count]\n",
    "              for count, y in enumerate(rf_prediction_without_probability_test)])/len(rf_prediction_without_probability_test))\n",
    "rf_prediction_with_probability = cross_val_predict(clf, biased_features_with_no_meaning, df2_train['sentiment'],\n",
    "                                                       method='predict_proba', verbose=3, n_jobs=1)\n",
    "\n",
    "propensity = np.array([rf_prediction_with_probability[i, y[i]] for i in range(len(rf_prediction_with_probability))])\n",
    "print(np.mean(np.log(propensity)))\n",
    "np.save('propensity.npy', propensity)\n",
    "\n",
    "# propensity = np.load(\"propensity.npy\")\n",
    "prob_1_l = np.array([(propensity[i] if y[i] == 1 else (1-propensity[i]))\n",
    "          for i in range(len(y))])\n",
    "prob_0_l = 1 - prob_1_l\n",
    "\n",
    "\n",
    "def calculate_weight_fraction(prob_1):\n",
    "    prob_0 = 1 - prob_1\n",
    "    w1 = 1 / (prob_0 * prob_1_l / (prob_0 * prob_1_l + prob_1 * prob_0_l))\n",
    "    w0 = 1 / (prob_1 * prob_0_l / (prob_0 * prob_1_l + prob_1 * prob_0_l))\n",
    "    return sum(w1[i] for i in range(len(y)) if y[i] == 1) / sum(w0[i] for i in range(len(y)) if y[i] == 0)\n",
    "\n",
    "\n",
    "prior_fraction = np.sum(y) / (len(y) - np.sum(y))\n",
    "l, r = 0, 1\n",
    "thr = 0.00000000001\n",
    "step = 100\n",
    "# while l + thr < r:\n",
    "for _ in range(step):\n",
    "    m1 = l + (r- l) / 2\n",
    "    if calculate_weight_fraction(m1) < prior_fraction:\n",
    "        l = m1\n",
    "    else:\n",
    "        r = m1\n",
    "\n",
    "m0 = 1 - m1\n",
    "w1 = 1 / (m0 * prob_1_l / (m0 * prob_1_l + m1 * prob_0_l))\n",
    "w0 = 1 / (m1 * prob_0_l / (m0 * prob_1_l + m1 * prob_0_l))\n",
    "weight_for_training_set = np.array([(w1[i] if y[i] == 1 else w0[i]) for i in range(len(y))])\n",
    "weight_for_training_set = weight_for_training_set / np.mean(weight_for_training_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "practical_model = Sequential()\n",
    "practical_model.add(Embedding(max_features, embed_size))\n",
    "practical_model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "practical_model.add(GlobalMaxPool1D())\n",
    "practical_model.add(Dense(20, activation=\"relu\"))\n",
    "practical_model.add(Dropout(0.05))\n",
    "practical_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "practical_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "practical_model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2, sample_weight=weight_for_training_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_performance_test(model, df2_train), model_performance_test(practical_model, df2_train)\n",
    "model_performance_test(model, df2_test), model_performance_test(practical_model, df2_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "difference_dict = {}\n",
    "new_difference_dict = {}\n",
    "for word in tqdm(words_with_no_meaning):\n",
    "    difference_dict[word] = np.mean(compute_effect_of_word(word, df2_test, df2_train, model))\n",
    "    new_difference_dict[word] = np.mean(compute_effect_of_word(word, df2_test, df2_train, practical_model))\n",
    "\n",
    "print(difference_dict)\n",
    "print(new_difference_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "width = 0.2\n",
    "x = list(range(len(words_with_no_meaning)))\n",
    "difference_array = np.abs(np.array(list(difference_dict.values())))\n",
    "new_difference_array = np.abs(np.array(list(new_difference_dict.values())))\n",
    "print(np.sum(difference_array>new_difference_array))\n",
    "plt.ylabel('Accuracy Difference (absolute value)')\n",
    "plt.bar(x, np.abs(np.array(list(difference_dict.values()))), width=width, label='Original Model',fc = 'r')\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i] + width\n",
    "plt.bar(x, np.abs(np.array(list(new_difference_dict.values()))), width=width, tick_label=words_with_no_meaning, label='Adjusted Model',fc = 'y')\n",
    "plt.legend()\n",
    "plt.savefig('./images/new_difference.png')\n",
    "print(pd.DataFrame.describe(pd.DataFrame(weight_for_training_set)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams['savefig.dpi'] = 400\n",
    "plt.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "plt.bar(x, np.abs(np.array(list(difference_dict.values()))), width=2*width, tick_label=words_with_no_meaning, label='Original Model',fc = 'blue')\n",
    "plt.ylabel('Accuracy Difference (absolute value)')\n",
    "plt.savefig('./images/difference.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_difference = 0\n",
    "new_total_difference = 0\n",
    "for word in words_with_no_meaning:\n",
    "    num = len(np.array(df2_test['review'][[word in i for i in df2_test['review']]]))\n",
    "    total_difference += num * difference_dict[word]\n",
    "    new_total_difference += num * new_difference_dict[word]\n",
    "\n",
    "print(total_difference, new_total_difference)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import numpy as np\n",
    "\n",
    "def create_biased_dataset(df, word):\n",
    "    biased_dataset = df[[word in i for i in df['review']]].reset_index().drop('index', axis=1)\n",
    "    biased_dataset = biased_dataset[[i == 1 for i in biased_dataset['sentiment']]].reset_index().drop('index', axis=1)\n",
    "    biased_dataset_neg = df[[word not in i for i in df['review']]].reset_index().drop('index', axis=1)\n",
    "    biased_dataset_neg = biased_dataset_neg[[i == 0 for i in biased_dataset_neg['sentiment']]].reset_index().drop('index', axis=1)\n",
    "    length = min(len(biased_dataset['review']), len(biased_dataset_neg['review']))\n",
    "    biased_dataset = pd.concat([biased_dataset[0:length], biased_dataset_neg[0:length]]).reset_index().drop('index', axis=1)\n",
    "    return biased_dataset\n",
    "\n",
    "def compute_effect_of_word_new(word, df, tested_model):\n",
    "    similar = '\\s'\n",
    "    df_chosen = np.array(df['review'][[word in i for i in df['review']]])\n",
    "    prediction1 = tested_model.predict(pad_sequences(tokenizer.texts_to_sequences(df_chosen), maxlen=maxlen))\n",
    "    for i, sentence in enumerate(df_chosen):\n",
    "        df_chosen[i] = sentence.replace(word, similar)\n",
    "    prediction2 = tested_model.predict(pad_sequences(tokenizer.texts_to_sequences(df_chosen), maxlen=maxlen))\n",
    "    return prediction1 - prediction2\n",
    "\n",
    "def biased_demo(df, word):\n",
    "    tmp_df = create_biased_dataset(df, word)\n",
    "    tmp_tokenizer = Tokenizer(num_words=max_features)\n",
    "    tmp_tokenizer.fit_on_texts(tmp_df['review'])\n",
    "    tmp_list_tokenized_train = tmp_tokenizer.texts_to_sequences(tmp_df['review'])\n",
    "    tmp_X_t = pad_sequences(tmp_list_tokenized_train, maxlen=maxlen)\n",
    "    tmp_y = tmp_df['sentiment']\n",
    "    tmp_model = Sequential()\n",
    "    tmp_model.add(Embedding(max_features, embed_size))\n",
    "    tmp_model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "    tmp_model.add(GlobalMaxPool1D())\n",
    "    tmp_model.add(Dense(20, activation=\"relu\"))\n",
    "    tmp_model.add(Dropout(0.05))\n",
    "    tmp_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    tmp_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    tmp_model.fit(tmp_X_t,tmp_y, batch_size=batch_size, epochs=3, validation_split=0.2)\n",
    "    tmp_difference_dict = compute_effect_of_word_new(word, df, tmp_model)\n",
    "    return tmp_difference_dict\n",
    "\n",
    "biased_difference_dict = {}\n",
    "org_difference_dict = {}\n",
    "for word in words_with_no_meaning:\n",
    "    biased_difference_dict[word] = np.mean(biased_demo(df2_train, word))\n",
    "    org_difference_dict[word] = np.mean(compute_effect_of_word_new(word, df2_train, model))\n",
    "\n",
    "print(biased_difference_dict)\n",
    "print(org_difference_dict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-d32dbc62",
   "language": "python",
   "display_name": "PyCharm (AI_project)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}