{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "# Modules for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Modules for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# Tools for preprocessing input data\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Tools for creating ngrams and vectorizing input data\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "\n",
    "# Tools for building a model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tools for assessing the quality of model prediction\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    return text\n",
    "\n",
    "df1 = pd.read_csv('./datasets/labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "df1 = df1.drop(['id'], axis=1)\n",
    "df2 = pd.read_csv('./datasets/imdb_master.csv',encoding=\"latin-1\")\n",
    "df2['review'] = df2.review.apply(lambda x: clean_text(x))\n",
    "df1['review'] = df1.review.apply(lambda x: clean_text(x))\n",
    "df2 = df2[df2.label != 'unsup']\n",
    "df2['label'].replace('neg', 0, inplace=True)\n",
    "df2['label'].replace('pos', 1, inplace=True)\n",
    "df2 = df2.drop(columns=[df2.keys()[0], df2.keys()[4]])\n",
    "df2 = df2.rename(columns={'label':'sentiment'})\n",
    "df2_train, df2_test = df2[df2['type']=='train'].drop(columns=['type']), df2[df2['type']=='test'].drop(columns=['type'])\n",
    "df2_train, df2_test = df2_train.reset_index(drop=True), df2_test.reset_index(drop=True)\n",
    "df1_test=pd.read_csv(\"./datasets/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "df1_test[\"sentiment\"] = df1_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "df1_test = df1_test.drop(['id'], axis=1)\n",
    "df1_test['review'] = df1_test.review.apply(lambda x: clean_text(x))\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIG_SIZE = 16\n",
    "LARGE_SIZE = 20\n",
    "params = {\n",
    "    'figure.figsize': (16, 8),\n",
    "    'font.size': SMALL_SIZE,\n",
    "    'xtick.labelsize': MEDIUM_SIZE,\n",
    "    'ytick.labelsize': MEDIUM_SIZE,\n",
    "    'legend.fontsize': BIG_SIZE,\n",
    "    'figure.titlesize': LARGE_SIZE,\n",
    "    'axes.titlesize': MEDIUM_SIZE,\n",
    "    'axes.labelsize': BIG_SIZE\n",
    "}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2_train['review_lenght'] = np.array(list(map(len, df2_train['review'])))\n",
    "median = df2_train['review_lenght'].median()\n",
    "mean = df2_train['review_lenght'].mean()\n",
    "mode = df2_train['review_lenght'].mode()[0]\n",
    "fig, ax = plt.subplots()\n",
    "sb.distplot(df2_train['review_lenght'], bins=df2_train['review_lenght'].max(),\n",
    "            hist_kws={\"alpha\": 0.9, \"color\": \"blue\"}, ax=ax,\n",
    "            kde_kws={\"color\": \"black\", 'linewidth': 3})\n",
    "ax.set_xlim(left=0, right=np.percentile(df2_train['review_lenght'], 95))\n",
    "ax.set_xlabel('Words in review')\n",
    "ymax = 0.014\n",
    "plt.ylim(0, ymax)\n",
    "ax.plot([mode, mode], [0, ymax], '--', label=f'mode = {mode:.2f}', linewidth=4)\n",
    "ax.plot([mean, mean], [0, ymax], '--', label=f'mean = {mean:.2f}', linewidth=4)\n",
    "ax.plot([median, median], [0, ymax], '--',\n",
    "        label=f'median = {median:.2f}', linewidth=4)\n",
    "ax.set_title('Words per review distribution', fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_file = './datasets/GoogleNews-vectors-negative300.bin'\n",
    "print(\"Loading word2vec model......\")\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(model_file,binary=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import thread_map\n",
    "def vectorize_data(data, vocab: dict) -> list:\n",
    "    print('Vectorize sentences...', end='\\r')\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(thread_map(encode, data, max_workers=2))\n",
    "    print('Vectorize sentences... (done)')\n",
    "    return vectorized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "pad_popcorn_test = pad_sequences(sequences=vectorize_data(df1_test['review'], vocab=wv_model.wv.vocab),maxlen=maxlen,padding='post')\n",
    "pad_popcorn_train = pad_sequences(sequences=vectorize_data(df1['review'], vocab=wv_model.wv.vocab),maxlen=maxlen,padding='post')\n",
    "pad_train = pad_sequences(sequences=vectorize_data(df2_train['review'], vocab=wv_model.wv.vocab),maxlen=maxlen,padding='post')\n",
    "pad_test = pad_sequences(sequences=vectorize_data(df2_test['review'], vocab=wv_model.wv.vocab),maxlen=maxlen,padding='post')\n",
    "\n",
    "np.save(\"./datasets/pad_train.npy\", pad_train)\n",
    "np.save(\"./datasets/pad_test.npy\", pad_test)\n",
    "np.save(\"./datasets/pad_popcorn_train.npy\", pad_popcorn_train)\n",
    "np.save(\"./datasets/pad_popcorn_test.npy\", pad_popcorn_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pad_train = np.load(\"./datasets/pad_train.npy\")\n",
    "pad_test = np.load(\"./datasets/pad_test.npy\")\n",
    "pad_popcorn_train = np.load(\"./datasets/pad_popcorn_train.npy\")\n",
    "pad_popcorn_test = np.load(\"./datasets/pad_popcorn_test.npy\")\n",
    "pad_train = np.array([pad_train[i] for i in range(pad_train.shape[0])])\n",
    "pad_test = np.array([pad_test[i] for i in range(pad_test.shape[0])])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "def build_model(embedding_matrix: np.ndarray, input_length: int):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim = embedding_matrix.shape[0],\n",
    "        output_dim = embedding_matrix.shape[1],\n",
    "        input_length = input_length,\n",
    "        weights = [embedding_matrix],\n",
    "        trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    embedding_matrix=wv_model.wv.vectors,\n",
    "    input_length=maxlen)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    x=pad_train,\n",
    "    y=df2_train['sentiment'],\n",
    "    validation_data=(pad_test, df2_test['sentiment']),\n",
    "    batch_size=100,\n",
    "    epochs=6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./models/org_model.pickle', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "print('Done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./models/org_model.pickle', 'rb') as file:\n",
    "   model=pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "def model_performance_test(tested_model, pad, y):\n",
    "    prediction = tested_model.predict(pad)\n",
    "    y_pred = (prediction > 0.5)\n",
    "    print('F1-score: {0}'.format(f1_score(y_pred, y)))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_pred, y))\n",
    "    print('Accuracy: {0}'.format(accuracy_score(y_pred, y)))\n",
    "    return f1_score(y_pred, y), confusion_matrix(y_pred, y), accuracy_score(y_pred, y)\n",
    "\n",
    "# model_performance_test(model, pad_test, df2_test['sentiment'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def word_existence_oracle(sentence, word):\n",
    "    return word in sentence\n",
    "\n",
    "def check_existence_in_train(word, df):\n",
    "    result = 0\n",
    "    for sentence in df['review']:\n",
    "        result += word_existence_oracle(sentence, word)\n",
    "    return result != 0\n",
    "\n",
    "def compute_effect_of_word(word, df, pad, tested_model):\n",
    "    if not check_existence_in_train(word, df2_test):\n",
    "        return None\n",
    "    index_chosen = []\n",
    "    for index, i in enumerate(df['review']):\n",
    "        if word in i:\n",
    "            index_chosen.append(index)\n",
    "    keys = list(wv_model.wv.vocab.keys())\n",
    "    embedding = keys.index(word)\n",
    "    tmp_pad1 = []\n",
    "    tmp_pad2 = []\n",
    "    for i in index_chosen:\n",
    "        tmp_pad1.append([0 if j == embedding else j for j in pad[i]])\n",
    "        tmp_pad2.append(pad[i])\n",
    "    tmp_pad1 = np.array(tmp_pad1)\n",
    "    tmp_pad2 = np.array(tmp_pad2)\n",
    "    prediction1 = tested_model.predict(tmp_pad2)\n",
    "    prediction2 = tested_model.predict(tmp_pad1)\n",
    "    return prediction1 - prediction2\n",
    "\n",
    "def word_summary(word, df, pad, tested_model):\n",
    "    difference = compute_effect_of_word(word, df, pad, tested_model)\n",
    "    print('mean, var of \\''+word+'\\': ', np.mean(difference), np.var(difference))\n",
    "    \n",
    "for i in ['movie', 'car', 'us', 'man', 'review', 'house']:\n",
    "    word_summary(i, df2_test, pad_test, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "words_with_no_meaning = ['it', 'this', 'are', 'is', 'was', 'will', 'that', 'my', 'there', 'be', 'with', 'in',\n",
    "                         'out', 'on', 'under', 'how', 'what', 'why', 'may', 'have', 'where', 'he', 'she', 'do',\n",
    "                         'when', 'were', 'these', 'those', 'can', 'could', 'has', 'had', 'them', 'would', 'which']\n",
    "\n",
    "nouns_can_be_removed = ['movie', 'house', 'film', 'car', 'tree']\n",
    "\n",
    "biased_words = words_with_no_meaning\n",
    "existence = [check_existence_in_train(word, df2_train) for word in biased_words]\n",
    "biased_words = list(filter(None, [biased_words[i] if existence[i] else None for i in range(len(existence))]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = df2_train['sentiment']\n",
    "def word_existence_list(word, df):\n",
    "    tmp_list = np.zeros(len(df['review']))\n",
    "    for count, sentence in enumerate(df['review']):\n",
    "        tmp_list[count] = float(word in sentence)\n",
    "    return tmp_list\n",
    "\n",
    "biased_features_with_no_meaning = pd.DataFrame()\n",
    "biased_features_with_no_meaning_test = pd.DataFrame()\n",
    "for words in words_with_no_meaning:\n",
    "    biased_features_with_no_meaning[words] = word_existence_list(words, df2_train)\n",
    "    biased_features_with_no_meaning_test[words] = word_existence_list(words, df2_test)\n",
    "clf = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "clf.fit(biased_features_with_no_meaning, df2_train['sentiment'])\n",
    "rf_prediction_without_probability = clf.predict(biased_features_with_no_meaning)\n",
    "rf_prediction_without_probability_test = clf.predict(biased_features_with_no_meaning_test)\n",
    "print('Train accuracy is ',\n",
    "      np.sum([y==df2_train['sentiment'][count]\n",
    "              for count, y in enumerate(rf_prediction_without_probability)])/len(rf_prediction_without_probability))\n",
    "print('Test accuracy is ',\n",
    "      np.sum([y==df2_test['sentiment'][count]\n",
    "              for count, y in enumerate(rf_prediction_without_probability_test)])/len(rf_prediction_without_probability_test))\n",
    "rf_prediction_with_probability = cross_val_predict(clf, biased_features_with_no_meaning, df2_train['sentiment'],\n",
    "                                                       method='predict_proba', verbose=3, n_jobs=1)\n",
    "\n",
    "propensity = np.array([rf_prediction_with_probability[i, y[i]] for i in range(len(rf_prediction_with_probability))])\n",
    "print(np.mean(np.log(propensity)))\n",
    "np.save('propensity.npy', propensity)\n",
    "\n",
    "prob_1_l = np.array([(propensity[i] if y[i] == 1 else (1-propensity[i]))\n",
    "          for i in range(len(y))])\n",
    "prob_0_l = 1 - prob_1_l\n",
    "\n",
    "\n",
    "def calculate_weight_fraction(prob_1):\n",
    "    prob_0 = 1 - prob_1\n",
    "    w1 = 1 / (prob_0 * prob_1_l / (prob_0 * prob_1_l + prob_1 * prob_0_l))\n",
    "    w0 = 1 / (prob_1 * prob_0_l / (prob_0 * prob_1_l + prob_1 * prob_0_l))\n",
    "    return sum(w1[i] for i in range(len(y)) if y[i] == 1) / sum(w0[i] for i in range(len(y)) if y[i] == 0)\n",
    "\n",
    "\n",
    "prior_fraction = np.sum(y) / (len(y) - np.sum(y))\n",
    "l, r = 0, 1\n",
    "thr = 0.00000000001\n",
    "step = 100\n",
    "\n",
    "for _ in range(step):\n",
    "    m1 = l + (r- l) / 2\n",
    "    if calculate_weight_fraction(m1) < prior_fraction:\n",
    "        l = m1\n",
    "    else:\n",
    "        r = m1\n",
    "\n",
    "m0 = 1 - m1\n",
    "w1 = 1 / (m0 * prob_1_l / (m0 * prob_1_l + m1 * prob_0_l))\n",
    "w0 = 1 / (m1 * prob_0_l / (m0 * prob_1_l + m1 * prob_0_l))\n",
    "weight_for_training_set = np.array([(w1[i] if y[i] == 1 else w0[i]) for i in range(len(y))])\n",
    "weight_for_training_set = weight_for_training_set / np.mean(weight_for_training_set)\n",
    "print(pd.DataFrame(weight_for_training_set).describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_model = build_model(\n",
    "    embedding_matrix=wv_model.wv.vectors,\n",
    "    input_length=maxlen)\n",
    "\n",
    "new_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "new_history = new_model.fit(\n",
    "    x=pad_train,\n",
    "    y=df2_train['sentiment'],\n",
    "    sample_weight=weight_for_training_set,\n",
    "    validation_data=(pad_test, df2_test['sentiment']),\n",
    "    batch_size=100,\n",
    "    epochs=6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_score_org = []\n",
    "accuracy_org = []\n",
    "f1_score_new = []\n",
    "accuracy_new = []\n",
    "a, _, c = model_performance_test(model, pad_popcorn_test, df1_test['sentiment'])\n",
    "f1_score_org.append(a)\n",
    "accuracy_org.append(c)\n",
    "a, _, c = model_performance_test(new_model, pad_popcorn_test, df1_test['sentiment'])\n",
    "f1_score_new.append(a)\n",
    "accuracy_new.append(c)\n",
    "a, _, c = model_performance_test(model, pad_popcorn_train, df1['sentiment'])\n",
    "f1_score_org.append(a)\n",
    "accuracy_org.append(c)\n",
    "a, _, c = model_performance_test(new_model, pad_popcorn_train, df1['sentiment'])\n",
    "f1_score_new.append(a)\n",
    "accuracy_new.append(c)\n",
    "a, _, c = model_performance_test(model, pad_test, df2_test['sentiment'])\n",
    "f1_score_org.append(a)\n",
    "accuracy_org.append(c)\n",
    "a, _, c = model_performance_test(new_model, pad_test, df2_test['sentiment'])\n",
    "f1_score_new.append(a)\n",
    "accuracy_new.append(c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_data = pd.DataFrame({'Accuracy':accuracy_org+accuracy_new,\n",
    "                              'Model':['Original Model' for i in range(3)]+['Debiased Model' for i in range(3)],\n",
    "                              'Datasets':['Bag of Words Meets Bags of Popcorn (test)',\n",
    "                                          'Bag of Words Meets Bags of Popcorn (train)',\n",
    "                                          'IMDB Review Dataset (test)',\n",
    "                                          'Bag of Words Meets Bags of Popcorn (test)',\n",
    "                                          'Bag of Words Meets Bags of Popcorn (train)',\n",
    "                                          'IMDB Review Dataset (test)']})\n",
    "sb.barplot(x='Datasets', y='Accuracy', hue='Model', data=accuracy_data)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "difference_dict = {}\n",
    "new_difference_dict = {}\n",
    "for word in tqdm(['movie', 'house', 'film', 'car', 'tree', 'which', 'would', 'could']):\n",
    "    difference_dict[word] = np.mean(compute_effect_of_word(word, df2_test, pad_test, model))\n",
    "    new_difference_dict[word] = np.mean(compute_effect_of_word(word, df2_test, pad_test, new_model))\n",
    "\n",
    "print(difference_dict)\n",
    "print(new_difference_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validate_word_list = ['movie', 'house', 'film', 'car', 'tree', 'which', 'would', 'could']\n",
    "difference_dataframe = pd.DataFrame({\n",
    "    'Words': validate_word_list + validate_word_list,\n",
    "    'Prediction Difference': [difference_dict[i] for i in validate_word_list] + [new_difference_dict[i] for i in validate_word_list],\n",
    "    'Model': ['Original Model' for i in validate_word_list] + ['Debiased Model' for i in validate_word_list]\n",
    "})\n",
    "plt.figure(figsize=(15, 11))\n",
    "sb.set(font_scale=2.4, style='white', )\n",
    "sb.barplot(x='Words', y='Prediction Difference', hue='Model', data=difference_dataframe,\n",
    "           palette=[(68/256,114/256,196/256), (237/256,125/256,49/256)]).set_title('The Effect of Words on Prediction')\n",
    "plt.savefig('./images/difference_comparison.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read data\n",
    "reviews_df = pd.read_csv(\"./datasets/Hotel_Reviews.csv\")\n",
    "# append the positive and negative text reviews\n",
    "reviews_df[\"review\"] = reviews_df[\"Negative_Review\"] + reviews_df[\"Positive_Review\"]\n",
    "# create the label\n",
    "reviews_df[\"sentiment\"] = reviews_df[\"Reviewer_Score\"].apply(lambda x: 1 if x < 9.75 else 0)\n",
    "# select only relevant columns\n",
    "reviews_df = reviews_df[[\"review\", \"sentiment\"]]\n",
    "reviews_df = reviews_df[np.array([(np.random.uniform(0, 50, 1)[0] < 0.5) for i in range(len(reviews_df))])]\n",
    "reviews_df['review'] = reviews_df.review.apply(lambda x: clean_text(x))\n",
    "reviews_df = reviews_df.reset_index()\n",
    "reviews_df = reviews_df.drop(['index'], axis = 1)\n",
    "pad_extended_test = pad_sequences(sequences=vectorize_data(reviews_df['review'], vocab=wv_model.wv.vocab),maxlen=maxlen,padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_performance_test(model, pad_extended_test, reviews_df['sentiment'])\n",
    "model_performance_test(new_model, pad_extended_test, reviews_df['sentiment'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_difference_dict = {}\n",
    "for word in tqdm(words_with_no_meaning):\n",
    "    try:\n",
    "        all_difference_dict[word] = np.mean(compute_effect_of_word(word, df2_test, pad_test, model))\n",
    "    except:\n",
    "        print(word)\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "width = 0.2\n",
    "plt.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "plt.bar(range(len(all_difference_dict)), np.abs(np.array(list(all_difference_dict.values()))), width=2*width, label='Original Model',fc = 'blue')\n",
    "plt.ylabel('Accuracy Difference (absolute value)')\n",
    "plt.savefig('./images/org_difference.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-d32dbc62",
   "language": "python",
   "display_name": "PyCharm (AI_project)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}